{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TooManyRedirects",
     "evalue": "Exceeded 30 redirects.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRedirects\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-484a15456c3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnewsurl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'https://baike.baidu.com/item/Python/407313?fr=aladdin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewsurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ToolsForEdit\\Anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_redirects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTooManyRedirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Exceeded %s redirects.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_redirects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Release the connection back into the pool.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTooManyRedirects\u001b[0m: Exceeded 30 redirects."
     ]
    }
   ],
   "source": [
    "import requests as req\n",
    "newsurl='https://baike.baidu.com/item/Python/407313?fr=aladdin'\n",
    "res=req.get(newsurl)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello World This is link1 This is link2  \n"
     ]
    }
   ],
   "source": [
    "##将网页镀金BeautifulSoup中\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "html_sample='\\\n",
    "<html>\\\n",
    " <body>\\\n",
    " <h1 id=\"title\"><a>Hello World</a></h1>\\\n",
    " <a href=\"#\" class=\"link\">This is link1</a>\\\n",
    " <a href=\"# link2\" class=\"link\">This is link2</a>\\\n",
    " </body>\\\n",
    " </html>'\n",
    "soup=bfs(html_sample,'html.parser')\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"><a>Hello World</a></h1>\n",
      "a[0]: Hello World\n",
      "[<h1 id=\"title\"><a>Hello World</a></h1>]\n"
     ]
    }
   ],
   "source": [
    "#使用select找出含有h1卷标的元素\n",
    "soup=bfs(html_sample,'lxml')\n",
    "header=soup.select('h1')\n",
    "for h in header:\n",
    "    print(h)\n",
    "    a = h.select('a')\n",
    "    print('a[0]:',a[0].text)\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "This is link1\n",
      "This is link2\n"
     ]
    }
   ],
   "source": [
    "#使用select找出含有a卷标的元素\n",
    "soup=bfs(html_sample,'lxml')\n",
    "alink=soup.select('a')\n",
    "for link in alink:\n",
    "    print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 id=\"title\">Hello World</h1>]\n"
     ]
    }
   ],
   "source": [
    "#使用select找出所有id为title的元素（id前面加#）\n",
    "alink=soup.select('#title')\n",
    "print(alink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link\" href=\"#\">This is link1</a>\n",
      "This is link1\n",
      "<a class=\"link\" href=\"# link2\">This is link2</a>\n",
      "This is link2\n"
     ]
    }
   ],
   "source": [
    "#找出所有class为link的元素（class前面加.)\n",
    "soup=bfs(html_sample,'lxml')\n",
    "for link in soup.select('.link'):\n",
    "    print(link)\n",
    "    print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# link2\n"
     ]
    }
   ],
   "source": [
    "#找出所有a tag的href链接\n",
    "alinks=soup.select('a')\n",
    "for link in alinks:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90后都工作了，“反向春运”高峰也就不稀奇了 https://new.qq.com/omn/20190111/20190111A18R8U.html\n"
     ]
    }
   ],
   "source": [
    "soup_t=bfs(res.text,'lxml')\n",
    "for ele in soup_t.select('.news-list'):\n",
    "    #print(ele.text)\n",
    "    print(ele.select('a')[0].text,ele.select('a')[0]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取澎湃新闻 简单爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craw 1 https://www.thepaper.cn/\n",
      "craw 2 https://www.thepaper.cn/channel_26916\n",
      "craw 3 https://www.thepaper.cn/gov_publish.jsp\n",
      "craw 4 https://www.thepaper.cn/gov_publish.jspchannel_36079\n",
      "craw 5 https://www.thepaper.cn/channel_25952\n",
      "craw 6 https://www.thepaper.cn/channel_25953\n",
      "craw 7 https://www.thepaper.cn/ask_index.jsp\n",
      "craw 8 https://www.thepaper.cn/channel_25953channel_25950\n",
      "craw 9 https://www.thepaper.cn/channel_25952/\n",
      "craw 10 https://www.thepaper.cn/channel_25952ask_index.jsp\n",
      "craw 11 https://www.thepaper.cn/channel_25952channel_25951\n",
      "craw 12 https://www.thepaper.cn/channel_25951\n",
      "craw 13 https://www.thepaper.cn/channel_26916channel_25953\n",
      "craw 14 https://www.thepaper.cn/ask_index.jsp/\n",
      "craw 15 https://www.thepaper.cn/channel_26916channel_36079\n",
      "craw 16 https://www.thepaper.cn/ask_index.jspchannel_36079\n",
      "craw 17 https://www.thepaper.cn/channel_25952channel_25953\n",
      "craw 18 https://www.thepaper.cn/gov_publish.jsp/\n",
      "craw 19 https://www.thepaper.cn/channel_25951channel_36079\n"
     ]
    }
   ],
   "source": [
    "### spiderMain\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "\n",
    "## url 管理器\n",
    "class UrlManager(object):\n",
    "    def __init__(self):\n",
    "        self.new_urls = set()\n",
    "        self.old_urls = set()\n",
    "    \n",
    "    def add_new_url(self,url):\n",
    "        if url is None:\n",
    "            return \n",
    "        if url not in self.new_urls and url not in self.old_urls:\n",
    "            self.new_urls.add(url)\n",
    "            \n",
    "    def add_new_urls(self,urls):\n",
    "        if urls is None or len(urls) == 0:\n",
    "            return \n",
    "        for url in urls:\n",
    "            self.add_new_url(url)\n",
    "            \n",
    "        \n",
    "    def has_new_url(self):\n",
    "        return len(self.new_urls) != 0\n",
    "        \n",
    "    def get_new_url(self):\n",
    "        new_url = self.new_urls.pop()\n",
    "        self.old_urls.add(new_url)\n",
    "        return new_url\n",
    "    \n",
    "    \n",
    "# 网页下载管理器\n",
    "class HtmlDownloader(object):\n",
    "    def download(slef,url):\n",
    "        if url is None:\n",
    "            return None\n",
    "        request = requests.get(url)\n",
    "        #print('HtmlDownloader:',request.text)\n",
    "        return request.text\n",
    "        \n",
    "# 网页格式管理器\n",
    "class HtmlParser(object):\n",
    "    def parse(self,page_url,html_cont):\n",
    "        if page_url is None or html_cont is None:\n",
    "            return \n",
    "        soup = bfs(html_cont,'html.parser')\n",
    "        new_urls = self._get_new_urls(page_url,soup)\n",
    "        new_data = self._get_new_data(page_url,soup)\n",
    "        #print(new_data[1])\n",
    "        return new_urls,new_data\n",
    "    \n",
    "    def _get_new_urls(self,page_url,soup):\n",
    "        new_urls = set()\n",
    "        links = soup.select('.bn_a')\n",
    "        for link in links:\n",
    "            new_url = link['href']\n",
    "            new_full_url = page_url+new_url\n",
    "            new_urls.add(new_full_url)\n",
    "        return new_urls\n",
    "            \n",
    "                                          \n",
    "    def _get_new_data(self,page_url,soup):\n",
    "        res_url=[]\n",
    "        res_title=[]\n",
    "        res_summary=[]\n",
    "        res_url.append(page_url)\n",
    "        for news in soup.select('.news_li'):\n",
    "            title_node = news.select('h2')\n",
    "            for title_n in title_node:\n",
    "                res_title.append(title_n.select('a')[0].text)\n",
    "            summary_node = news.select('p')\n",
    "            for summary_n in summary_node:\n",
    "                res_summary.append(summary_n.text)\n",
    "        return res_url,res_title,res_summary\n",
    "        \n",
    "# 网页输出 管理器\n",
    "class HtmlOutputer(object):\n",
    "    def __init__(self):\n",
    "        self.datas=[]\n",
    "    \n",
    "    def collect_data(self,data):\n",
    "        if data is None:\n",
    "            return \n",
    "        self.datas.append(data[0])\n",
    "        self.datas.append(data[1])\n",
    "        self.datas.append(data[2])\n",
    "        \n",
    "    def output_html(self):\n",
    "        fout = open('output.html','w')\n",
    "        fout.write('<html>')\n",
    "        fout.write('<body>')\n",
    "        fout.write('<table>')\n",
    "        i = 0 \n",
    "        for data in self.datas:\n",
    "            fout.write('<tr>')\n",
    "            if i%3 == 0:\n",
    "                fout.write('<th>'+data[0]+'</th>')\n",
    "            elif i%3 == 1 or i%3 == 2 :\n",
    "                for msg in data:\n",
    "                    fout.write('<td>'+msg+'</td>')\n",
    "            fout.write('</tr>')\n",
    "            i = i + 1\n",
    "                \n",
    "        fout.write('</table>')\n",
    "        fout.write('</body>')\n",
    "        fout.write('</html>')\n",
    "\n",
    "# 爬虫主程序\n",
    "class SpiderMain(object):\n",
    "    def __init__(self):\n",
    "        self.urls = UrlManager()\n",
    "        self.downloader = HtmlDownloader()\n",
    "        self.parser = HtmlParser()\n",
    "        self.outputer = HtmlOutputer()\n",
    "        \n",
    "    def craw(self,root_url):\n",
    "        count = 1 \n",
    "        self.urls.add_new_url(root_url)\n",
    "        while self.urls.has_new_url():\n",
    "            try:\n",
    "                new_url = self.urls.get_new_url()\n",
    "                html_cont = self.downloader.download(new_url)\n",
    "                new_urls,new_data = self.parser.parse(new_url,html_cont)\n",
    "                self.outputer.collect_data(new_data)\n",
    "\n",
    "                self.urls.add_new_urls(new_urls)\n",
    "                print('craw', count,new_url)\n",
    "                count = count + 1\n",
    "                if count == 20:\n",
    "                    break\n",
    "            except:\n",
    "                print('craw failed',count,new_url)\n",
    "\n",
    "        self.outputer.output_html()\n",
    "\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    root_url='https://www.thepaper.cn/'\n",
    "    obj_spider = SpiderMain()\n",
    "    obj_spider.craw(root_url)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬去 17k小说"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入数号：45\n",
      "网址：http://www.17k.com/list/45.html\n",
      "小说将保存到代码所在目录！\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-731dc6dd117b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'网址：'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'小说将保存到代码所在目录！'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'正在爬取小说：《%s》'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mshu_ming\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mfb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s.txt'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mfb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshu_ming\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "shuhao=str(input('请输入数号：'))\n",
    "url='http://www.17k.com/list/'+shuhao+'.html'\n",
    "r=requests.get(url)\n",
    "r.encoding=\"utf-8\"\n",
    "#print(r.text)\n",
    "tt=re.findall(r'<h1 class=\"Title\">(.*?)</h1>',r.text)\n",
    "zhanjie=re.findall(r'<a target=\"_blank\" href=\"(.*?)\" title=',r.text)\n",
    "shu_ming=re.findall(r'<meta name=\"Keywords\" content=\"(.*?),',r.text)\n",
    "if len(tt)==0:exit('错误的书号')\n",
    "print('网址：'+url)\n",
    "print('小说将保存到代码所在目录！')\n",
    "print('正在爬取小说：《%s》'%shu_ming[0])\n",
    "fb=open('%s.txt'%tt,'w',encoding='utf-8')\n",
    "fb.write(shu_ming[0])\n",
    "for m in zhanjie:\n",
    "    #print(m)\n",
    "    m='http://www.17k.com/'+m\n",
    "    #print(m)\n",
    "    sxnr=requests.get(m)\n",
    "    sxnr.encoding = 'utf-8'\n",
    "    #print(sxnr.text)\n",
    "    sxzw=re.findall(r'<div class=\"p\">([\\s\\S]*?)<div class=\"author-say\"></div>',sxnr.text)[0]\n",
    "    sxzj=re.findall(r'<h1>([\\s\\S]*?)</h1>',sxnr.text)[0]\n",
    "    sxzw=sxzw.replace('\\n&#12288;&#12288;','')\n",
    "    sxzw = sxzw.replace('<br />','\\n')\n",
    "    sxzw = sxzw.replace('&#12288;&#12288;', '')\n",
    "    sxzw = sxzw.replace('&quot;', '')\n",
    "    fb.write(sxzj)\n",
    "    fb.write('\\n')\n",
    "    fb.write(sxzw)\n",
    "    fb.write('\\n')\n",
    "    fb.write('\\n')\n",
    "    #print(sxzj)\n",
    "    print('>',end='')\n",
    "print('爬取完毕！')\n",
    "fb.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  git add1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
